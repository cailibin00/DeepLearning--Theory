## get
文章启发我们，可以从数学角度，人为干涉模型训练的权重，从而达到人认为的更优模型

## 前言
这是一篇针对于行人ID持久化识别的文章
从该角度切入，该论文重点改进了注意力**特征重复**的弊端，从而改进了注意力机制。
虽然他是针对于特定任务，但是对于一般的注意力方法有启示作用。
![[Pasted image 20250928144256.png|500]]
在行人识别之中，有一个gallery（行人数据库），还有我们要查找的token（输入图像），我们要根据token，在数据库之中重新查找相同的行人。
模型的输出，就是将所有的图片编码到一个低维向量query，和gallery所有向量进行欧氏几何的距离测算，得到最相近的向量，从而能够匹配。

对于模型，注意力机制会聚焦于行人的特定特征，但是，注意力并不会自发的多样化聚焦的特征，而是像论文中说的：
**The low feature correlation property is, however, not naturally guaranteed by attention-based models. Our observation is that those attention-based models are often more prone to higher feature correlations, because intuitively, the attention mechanism tends to have features focus on a more compact subspace** 
模型往往会趋向于高相关性的输出（低维向量之中的多个元素是重复的，降低了模型的性能）【上图可见】。
然而，行人识别的任务包含很多变化的因素：行人身体姿态，摄像头角度，身体错位。所以要是只关注某几个特定的位置（例如，衣服颜色，身体姿态）而不是整个行人的综合特征，就不够好。


## Attention
![[Pasted image 20250928145402.png]]

### CAM
这是一个通道为元素的注意力计算
`C*C` 矩阵可以第一行可以看作第一个通道对于其他通道的注意力关系，其他行同理。
矩阵相乘得到的`C*N` 看作，每一个通道在融合其他通道的注意力之后，产生的偏移量，
最后相加就是加上偏移量。

### PAM
他是针对于一个像素点的注意力计算
它输出矩阵`N*N`含义每一列的含义是，将一个通道的二维像素点展开成为一维像素点之后的第一个像素点对于其他N（包含本身）的像素点的注意力相关性。
相乘之后的`C*N` , N的每一个像素表示每一个原图像素点在其他像素点注意力之后的偏移量。

## 正交正则化
![[Pasted image 20250928161155.png]]
“正交正则化”的本质，就是通过数学约束，让特征嵌入的各维度“尽量独立、不重叠”，即实现“多样性”。 
### 现有正交方法的缺陷
当时两种主流“正交约束”方法的问题

#### 1. 硬正交约束
- **核心定义**：通过严格的数学约束，强制特征或权重满足 “正交性”（即不同维度 / 权重向量的内积为 0，信息完全独立），通常依赖**奇异值分解（SVD）** 将参数限制在 “Stiefel 流形”（数学上定义正交向量组的空间）上。
- **致命缺陷**（）：
    1. **计算成本极高**：对高维矩阵（如 Re-ID 中常用的 1024 维特征、2048 维权重）做 SVD 分解，**耗时严重**，难以适配大规模数据集（如 MSMT17）的训练；
    2. **限制模型灵活性**：“严格正交” 相当于给模型参数 “绑死” 了约束，可能导致模型无法学到更灵活的判别性特征（比如为了满足正交，被迫放弃部分行人关键信息）。

#### 2. 软正交约束
为解决硬约束的 “计算贵、不灵活” 问题，后续方法提出 “软约束”—— 不强制严格正交，而是通过损失函数引导参数 “尽量接近正交”，但仍存在偏差（）。
$$Gram = A A^T$$
- **代表方法与思路**：
    - 方法 1（Frobenius 范数约束）：强制 “特征 / 权重的 Gram 矩阵”，接近 “单位矩阵”（单位矩阵意味着各维度完全独立），用 Frobenius 范数计算 “Gram 矩阵与单位矩阵的差距” 作为损失；
    - 方法 2（谱范数约束）：用 “谱范数”（Gram 矩阵最大奇异值）做约束，缓解 Frobenius 范数的偏差。
- **核心缺陷**：
    - 实际信息没有那么多：例如行人 Re-ID 的特征嵌入常为 2048 维，**但实际有效信息维度远低于 2048**，导致 Gram 矩阵 “秩不足”—— 无论如何优化，**都不可能接近单位矩阵**，最终约束失效；

简单说：**硬约束“太贵太死板”，软约束“不准”**，这就是论文要设计新正交正则化的原因。

### O.F. + O.W. 
论文提出的正交正则化，核心是**同时对“特征（O.F.）”和“权重（O.W.）”做约束**，且用更高效、无偏差的“软约束”（SVDO）替代现有方法。 

#### 计算办法
统一操作——把“特征/权重”变成“矩阵” 无论是对特征还是权重做正交约束
- **Reshape**（重塑）：把高维的特征图/卷积核，变成二维矩阵。
	 对于特征   $F = R^{C,H,W} -> R^{C,W^*}$
	 对于权重（卷积） $F = R^{O , C , H , W} -> R^{O,C^*}$
- **正交约束**：得到gram矩阵
	 $Gram = AA^T$
- **高效计算：幂迭代法近似特征值**：直接计算高维 Gram 矩阵的特征值（EVD）仍耗时，文档采用**幂迭代法**（Power Iteration）做 “近似计算”，大幅降低成本：
    1. 初始化随机向量 q；
    2. 迭代更新：$(p \leftarrow X q)，(q \leftarrow X p)$   $(X=F F^T)$，默认迭代 2 次）；
    3. 近似最大特征值：$(\lambda_1 \approx \frac{| q |}{| p |})$；
    4. 近似最小特征值：将 X 替换为 $(F F^T - \lambda_1 I)$（I 为单位矩阵），重复上述步骤得到 \($\lambda_2$\)。


- **SVDO** ：不强制 Gram 矩阵接近单位矩阵，而是直接约束 “**Gram 矩阵的最大 - 最小特征值差**”
SVDO 正则化损失公式如下（）：$$\beta \cdot \| \lambda_1(F F^T) - \lambda_2(F F^T) \|_2 ^ 2$$
- $\beta$：正则化系数（控制约束强度，文档中 $\beta_{O.F.}=10^{-6}$，$\beta_{O.W.}=10^{-3}$
- $\lambda_1(F F^T)$、$\lambda_2(F F^T)$：分别为 Gram 矩阵 $F F^T$ 的最大、最小特征值；
- 目标：通过最小化此损失，让 $\lambda_1$ 与 $\lambda_2$ 尽可能接近，实现特征 / 权重的多样性。

#### 应用结构
ABD-Net 的正交正则化布局于特定的部分：
- **O.F.（特征正交）**：
    1. ResConv2 输出后（CAM 模块之后）：**约束通道聚合后的特征**，避免早期特征冗余；
    2. 注意力分支 ResConv5 输出后：约束细粒度注意力特征，确保最终**特征嵌入**的低相关性。

- **O.W.（权重正交）**：
    作用于 ResNet-50 的**所有卷积层**（ResConv1 到 ResConv5，包括全局分支和注意力分支的 ResConv5），确保每一层的滤波器都学到多样特征。

#### 原理解释
特征嵌入通常是 **过完备** 的 —— 即特征维度（对应特征值数量）远大于 有效信息维度。
例如，文档中 ABD-Net 的最终特征嵌入为 2048 维，但实际能区分不同行人的有效信息维度可能仅几十维，剩余维度均为冗余。
仅聚焦 “最大 - 最小特征值的差距”，恰好规避了 “过完备特征下数量虚高” 的问题 —— 无论总特征值有多少，只要最大与最小特征值差距小，就能确保 “有效特征维度贡献均衡”