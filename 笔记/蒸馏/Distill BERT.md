## 前言
当前的大模型以及朝着越来越大规模发展了，造成了难以在移动端部署的现象。前人早已经提出，使用蒸馏的手段能够简化模型，这以及被广泛在各个模型上验证，并且有多种蒸馏的手段提出。
本文致力于蒸馏BERT。

## RoBERTa
这是针对于BERT的，模型架构完全不变，训练策略的极大改进

### 1. 保留MLM
BERT的预训练任务包括NSP，MLM。
作者认为NSP任务无法适用于如此大规模的模型，任务太过于简单。
（我在实际的训练之中也发现了这一个点，他不会训练模型学习语法，词性等重要的文本理解，而是带模型走向通过**语义简单关联**判断上下文是否关联的一个极端，这不利于模型训练）

因此在训练过程之中直接丢弃NSP任务。

### 2. 动态掩码
这是一次极为大胆的创新。
传统BERT在完成数据处理之后（掩码和句子拼接），掩码保持不变，静态的文本掩码会在每一个batch之中不断训练循环。

动态掩码将**每一次输入**的文本掩码都做变化。
- 不会过拟合：每一次输入的数据都可能会不同，模型没办法通过记忆来拟合结果，而是被迫熟悉内部的逻辑关系
- 扩大了数据集：模型在扩大了的数据集上不断通过新的例题来学习，而不是反复刷题。
这个改动很经典，值得学习。

### 3. 更好的训练数据
16G -> 160G

BERT 的训练数据只有两类（共 16GB）：
- 英文维基百科（约 5GB）；
- 多伦多图书语料库（Toronto Book Corpus，约 11GB）—— 文本类型单一，覆盖场景有限。

RoBERTa 把数据量扩大到**160GB**，新增了 3 类更丰富的语料：
- CC-News（新闻语料，约 76GB）：涵盖全球新闻，能学正式书面语、时事相关表达；
- OpenWebText（网页文本，约 38GB）：从 Reddit 高赞链接爬取的网页，包含博客、论坛、文章，贴近日常语言；
- Stories（故事文本，约 31GB）：包含小说、短篇故事，能学叙事性语言、人物对话逻辑。

→ 效果：模型接触的语言场景从 “图书 + 维基” 扩展到 “新闻 + 网页 + 故事”，能处理更复杂、更多样的文本（比如新闻的客观表述、论坛的口语化表达）。


优化文本截断策略
**通过优化文本打包策略，在有限的输入长度内，尽可能多地保留语义上相关的上下文信息，从而让模型更好地理解那些相距较远的词之间的关系。**
1. **跨句子拼接**：它可能会把一个句子的后半部分和下一个句子的前半部分拼在一起，只要它们在语义上是连续的。
2. **跨文档拼接**：如果一个文档的结尾和另一个主题相关的文档的开头放在一起更合适，它也会这么做。

### 4. 更大的batch
256 -> 8000
作者认为，大批次更能稳定梯度下降，也防止了噪点带来的影响。

### 5. NSP分词
用NSP分词代替wordPiece分词。使得词汇相对来说更加分散。减少OVO

## 蒸馏
![[Pasted image 20250929202313.png|500]]
### 学生模型

**结构**
研究表明，在参数预算固定的情况下，张量**最后一维**（即全连接的隐藏层维度）的变化对计算效率的影响，小于层数等其他因素的变化，因此用减少层数来作为简化模型的主要手段
- 去掉 “token-type 嵌入”（用于区分句子对，如 BERT 的 \[SEP\] 标签相关嵌入）；
- 去掉 “pooler 层”（用于生成句子级表示，对多数下游任务非必需）；
- 层数减半：BERT-base 有 12 层 Transformer , Distill BERT 保留 6 层。

**初始化**
通过隔层抽取的方式从教师模型中初始化学生模型 —— 即从教师模型的层中每间隔一层选取一层，作为学生模型对应层的初始参数。
这是一个不改变隐藏层维度的好反馈。这也是一个人为干预训练过程的手段。

文章认为，这样子能够起到类似于蒸馏：教师带学生走出局部最小值的效果


### 蒸馏方式

包含三种损失函数，蒸馏损失和MLM损失 + **余弦嵌入损失**

- 余弦嵌入损失
针对的不是注意力图，而是transformer层之间的输出，具体来说，每两层教师模型的输出对应一层学生模型的输出。
计算公式
$$L_{cos} = 1 - cos\theta$$
$$\theta = \frac{h_1 \cdot h_2}{\|h_1\| \|h_2\|} $$
