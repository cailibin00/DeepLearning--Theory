源论文链接：[Attention and feature transfer based knowledge distillation | Scientific Reports](https://www.nature.com/articles/s41598-023-43986-y)
这不是一篇很厉害的文章。
## 预知识
### 一、AT

#### 1. **核心定义**

AT（文献 30）是**首个将注意力图迁移引入知识蒸馏（KD）的经典模型**，通过强制学生网络模仿教师的**类激活图（CAM）**，让学生学习 “教师在分类时关注的关键区域”。其核心逻辑是：**注意力图（推理过程）的迁移能提升学生对 “有效信息” 的聚焦能力**。

#### 2. **技术实现**
- **CAM 生成**：
    AT 基于 CNN 的全连接层权重，将其投影回**最后一层卷积特征图**，生成反映 “类别判别区域” 的 CAM。公式为：$$CAM_j(x, y) = \sum_{i=1}^C W_i^j A_i(x, y)$$
    其中，$W_i^j$是全连接层第j类对第i通道的权重，$A_i$是卷积特征图。CAM 通过像素值高低表示 “该区域对类别j的重要性”。
- **注意力迁移**：
    教师将 CAM 作为 “注意力图” 传递给学生，学生通过最小化 “师生 CAM 的 MSE 损失”，学习教师的关注模式。AT 的损失函数为：$$L_{AT} = \sum_{k} \frac{1}{C_k H_k W_k} \left\| \frac{\text{Norm}(CAM_{teacher}^k)}{\|\text{Norm}(CAM_{teacher}^k)\|_2} - \frac{\text{Norm}(CAM_{student}^k)}{\|\text{Norm}(CAM_{student}^k)\|_2} \right\|_2^2$$
#### 3. **贡献与局限**

- **贡献**：首次证明 “迁移推理过程（注意力）” 的有效性，为注意力基 KD 奠定基础；在 CIFAR-100 上，AT 使学生（VGG8）准确率达 71.43%（相同师生架构，表 5）。
- **局限**：仅迁移最后一层的 CAM，忽略中间层的连续推理过程；未对 CAM 进行二值化等优化，注意力图的 “关键区域” 不够突出。

### 二、CAT-KD

#### 1. **核心创新**
CAT-KD（文献 19，CVPR 2023）是**当时注意力基 KD 的先进方法**，针对 AT 的局限提出两点改进：
- **卷积替代 CAM**：
    通过 **1×1 卷积**生成更精细化的 注意力（非单纯 CAM），并通过**二值化操作**（0/1 掩码）突出教师关注的核心区域，避免学生学习到模糊的弱相关区域。
- **中间层连续迁移**：
    将 CAM 的生成推广到 CNN 的**所有中间层**（不仅最后一层），强制学生模仿教师在不同推理阶段的注意力分布，捕捉 “从局部到全局” 的连续关注逻辑。

#### 2. **技术细节（基于文档实验部分）**
- **二值化 CAM**：
    对教师中间层的预激活特征（Pre-Feature）生成 CAM 后，通过阈值二值化（如 0.5），仅保留教师 “最关注的区域”（白色），过滤无关背景（黑色），提升注意力图的针对性。
- **自适应损失权重**：
    在 CIFAR-100 实验中，CAT-KD 通过调整 “注意力损失权重 β”（如 1~50），平衡 CE 损失与注意力损失，避免单一损失主导训练。

#### 3. **性能表现**
- 在 CIFAR-100 上，当教师为 ResNet32×4、学生为同架构时，CAT-KD 学生准确率达 76.91%，优于 AT 的 73.44%（表 5）；
- 当师生架构不同（如教师 ResNet32×4→学生 ShuffleNetV1），CAT-KD 准确率 78.26%，仅略低于 AFT-KD 的 78.39%（表 4），证明其竞争力。

## AFD
![](file-20250930160432919.png)

![](file-20250930160522469.png )

人们总在蒸馏的路上越来越严格。
从一开始的标签蒸馏，到特征图蒸馏。到本论文之中认为仅仅是外化内容的蒸馏不能满足学到**推理过程**的需求，要求把这个推理的过程一起引导学生学习，但是这个推理过程物理意义上存在于教师模型的参数逻辑之中，不能外化。所以只能另辟蹊径，寻找简单的代替品。然而，这些不严谨的逻辑，也给了我对于本文章不太好的印象。

### 推理过程和结果

强调了，模型在传统KD（AT+KD）之中，直接学习特征图是学习老师推理之后的结果，那现在要学习老师推理的过程。

推理的过程认为可以体现在$Pre\_F$ 之中
$$F = ReLU(Pre\_F)$$
所以推理过程的信息集中在这个preF之中

利用这个preF，在经过上图二的步骤，最后与preF相加，就是推理过程和推理结果的融合了。

$$CAM = Binary(Conv1*1(Pre\_F))$$
$$AFD = CAM + ReLU(Pre\_F) $$
### 动态权重

$$L_{KD} = \alpha L_{cls} + \beta L_{AFD}$$
$$Dr\_{CE} = L\_CE / L^{0}\_CE$$
$$Dr\_AFD = L\_{AFD} / L^0\_AFD$$
$$\alpha = Dr\_CE / \frac{Dr\_{CE} + Dr\_{AFD}}{2}$$
$$\beta = Dr\_AFD / \frac{Dr\_{CE} + Dr\_{AFD}}{2}$$
- $L^0$ 表示训练第一个epoch得到的损失值
- $Dr$ 是一个衰减比率
核心思想是，要是这个衰减比例越小，就代表训练的越好，需要的权重就动态调整的越小。

### 缺点
现有的蒸馏严重依赖与模型的相似性，也就是存在契合的几个层用来AFD，否则只能是简单的认为对齐。
更加严重的是，有时候还会用到插值法或者1 * 1卷积来消除两边特征图的差距，这样有损性能。
不同架构的网络没办法实现蒸馏/效果很差。
