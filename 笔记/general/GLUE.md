### 1. GLUE（General Language Understanding Evaluation）

#### **从何而来？**

- **发布时间**：2018年，由纽约大学、华盛顿大学和DeepMind的研究者联合推出。
    
- **背景与动机**：在2018年之前，NLP领域是“碎片化”的。不同的子任务（如情感分析、释义判断、自然语言推理等）都有各自的数据集和排行榜。这使得很难衡量一个模型是否真正获得了**通用的语言理解能力**，还是仅仅在某个特定任务上过拟合了。
    
- **目标**：GLUE旨在创建一个**统一的、全面的**评估基准，将多个不同的NLP任务集合在一起，通过一个单一的分数来综合衡量模型的通用语言理解能力。它极大地推动了预训练语言模型（如BERT）的发展，成为了当时衡量模型性能的“黄金标准”。
    

#### **原理是什么？**

GLUE的核心原理是 **“多任务评估”** 和 **“难易集合”**。

1. **多任务评估**：GLUE包含了9个不同的自然语言理解任务，这些任务涵盖了多种理解技能：
    
    - **自然语言推理 (NLI)**：判断一个句子与另一个句子的关系（蕴含、矛盾、中立）。例如：MNLI, QNLI, RTE。
        
    - **情感分析**：判断句子的情感极性。例如：SST-2。
        
    - **释义判断**：判断两个句子是否表达相同的意思。例如：MRPC。
        
    - **语义相似度**：判断两个句子的语义相似度（打分）。例如：STSB。
        
    - **指代消解**：判断句子中的代词指代的是哪个名词。例如：WNLI。
        
    - **语言学可接受性判断**：判断一个句子在语法上是否可接受。例如：CoLA。
        
2. **难易集合**：这9个任务有难有易，既有分类也有回归，既有句子级也有句子对任务。一个强大的模型应该在**所有任务**上都表现出色，而不是只擅长某一类。这迫使模型学习更通用、更鲁棒的语言表示。
    

#### **如何计算？**

GLUE的得分不是一个简单的平均，而是分为两个部分：

1. **各子任务得分**：每个任务都有其自己的评估指标。
    
    - **分类任务**（如MNLI, SST-2）：使用**准确率（Accuracy）**。
        
    - **回归任务**（如STSB）：使用**皮尔逊相关系数（Pearson Correlation）**。
        
    - **匹配任务**（如MRPC, QQP）：使用**F1分数** 和**准确率**的**平均值**（具体取决于任务）。
        
    - **语言学可接受性（CoLA）**：使用**马修斯相关系数（Matthew‘s Correlation, MCC）**，适用于类别不均衡的二分类问题。
        
2. **总体得分（GLUE Score）**：
    
    - 将所有9个任务的得分**等权平均**，得到一个0到100之间的总分。
        
    - **计算公式**：`GLUE Score = Average( 所有子任务的得分 )`
        
    - 需要注意的是，由于WNLI任务设计特殊且很多模型在其上表现反常，有时在非官方比较中会被忽略，只计算另外8个任务的平均分（但官方标准始终是9个）。
        

**示例**：如果一个模型在9个任务上的得分分别是 [85, 90, 80, 75, 95, 88, 82, 79, 84]，那么它的GLUE Score就是这些数的平均值。

---

### 2. SuperGLUE

#### **从何而来？**

- **发布时间**：2019年底，由原来的GLUE团队和一些新成员推出。
    
- **背景与动机**：随着BERT等强大模型的出现，GLUE基准的排行榜迅速被“刷爆”，人类表现（Human Performance）被多个模型超越。这意味着GLUE已经不足以区分最顶尖模型的能力了。
    
- **目标**：创建一个**更困难、更偏向推理**的基准，其任务设计更接近人类真正的语言理解能力，从而为下一代模型提供新的挑战。
    

#### **原理是什么？**

SuperGLUE在GLUE原理的基础上，进行了如下升级：

1. **任务更难**：精心挑选了8个对人类来说很容易，但对机器来说更具挑战性的任务。这些任务需要更强的**逻辑推理、常识推理和指代消解**能力。
    
2. **更丰富的任务类型**：引入了需要**多句推理**和**知识问答**的任务。
    
3. **包含人类表现作为上限**：在设计基准时，同时收集了**人类标注者的表现数据**，为每个任务提供了一个明确的、可靠的人类水平上限，便于衡量模型与人类能力的差距。
    
4. **统一的评估指标**：尽可能使用更统一的指标（如准确率），简化比较。
    

SuperGLUE包含的8个任务：

- **BoolQ**：根据一段文本回答是/否问题。
    
- **CB**：包含因果和时序关系的自然语言推理。
    
- **COPA**：选择给定句子的原因或结果（常识推理）。
    
- **MultiRC**：从段落中回答多个问题，每个问题可能有多个正确答案。
    
- **ReCoRD**：从新闻句子中填空，需要从实体列表中选择正确答案（指代消解）。
    
- **RTE**：自然语言推理（与GLUE中的类似，但数据不同）。
    
- **WiC**：判断一个多义词在上下文中是否被用于相同的含义。
    
- **WSC**：判断一个代词在句子中指代的是哪个名词（核心指代消解）。
    

#### **如何计算？**

计算方式与GLUE类似，但细节有所不同。

1. **各子任务得分**：
    
    - 大部分任务（BoolQ, CB, COPA, RTE）使用**准确率（Accuracy）**。
        
    - **MultiRC**：使用**F1a分数**，这是一种兼顾问题层面和答案层面的综合指标。
        
    - **ReCoRD**：使用**F1分数** 和**准确率**的最大值。
        
    - **WiC**：使用**准确率**。
        
    - **WSC**：使用**准确率**。
        
2. **总体得分（SuperGLUE Score）**：
    
    - 将所有8个任务的得分进行**等权平均**，得到一个0到100之间的总分。
        
    - **计算公式**：`SuperGLUE Score = Average( 所有子任务的得分 )`
        

### 总结与对比

|特性|GLUE|SuperGLUE|
|---|---|---|
|**推出时间**|2018年|2019年底|
|**核心目标**|创建**统一基准**，评估**通用语言理解**|提供**更难的基准**，挑战**推理能力**，逼近人类水平|
|**任务数量**|9个|8个|
|**任务难度**|相对较低，已被BERT等模型大幅超越|更高，需要更强的逻辑和常识推理|
|**主要指标**|因任务而异（Acc, F1, MCC, Pearson）|更统一，主要使用**准确率（Accuracy）**|
|**人类基线**|未正式系统性地提供|**包含**，作为性能上限|
|**计算方式**|**所有任务得分的等权平均**|**所有任务得分的等权平均**|

### 现状与演进

- **现状**：如今，无论是GLUE还是SuperGLUE，其排行榜都已经被诸如**T5、DeBERTa、GPT**系列等强大模型“征服”，分数远超人类基线。这意味着它们作为研究前沿标杆的使命已经基本完成。
    
- **演进**：社区已经推出了**更复杂、更具挑战性**的新基准，例如：
    
    - **MMLU (Massive Multitask Language Understanding)**：涵盖57个学科（从数学、历史到法律、伦理）的常识和知识问答。
        
    - **BIG-bench**：一个超大规模的、社区合作的基准，包含200多个极其困难的任务。
        
    - **HELM (Holistic Evaluation of Language Models)**：不仅评估准确性，还评估模型的**校准度、鲁棒性、偏见、毒性**等多个维度，进行全方位评估。