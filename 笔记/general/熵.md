## 介绍

- 由于我们经常在损失函数之中使用到熵这个概念，我觉得有必要按照我的理解谈一下这个玩意以及他的数学公式

***熵***：从物理学来讲，是衡量一个/些物体的杂乱程度。在机器学习之中，他表示**概率分布的不确定性，或者分散程度**。
分布越分散，熵值越大，反之。

**交叉熵：** 衡量一个物体拟合另外一个物体的贴合程度。交叉表示有两个

## 熵的数学公式
$$H(p) = -\sum_{i=1}^{N}{p_i\ log(p_i)}$$

该公式由**香农**推导出来的**唯一满足信息论核心公理的表达式**

### 条件一：
**连续性**，不能由于微小突变发生突变

### 条件二：
**等概率分布时，不确定性随着类别数增加而单调递增**
在所有类别分布相等时，类别增加会导致不确定性增加，三选一和二选一不确定性不同
进一步，香农规定，“等概率分布的熵”，应该与  $logN$  成正比（N为类别数）
$$N = 2^k$$
- N为类别数，K为**比特信息**
- N个类别需要k的比特信息。如$N = 4 = 2^2$
因此有
$$H(1/N , 1/N , 1/N ...) = C \cdot logN$$
也就是，随着类别的增加，混乱程度应该时亚线性增长的。

### 条件三：
**无信息类别不增加不确定性**
如果一个类别$p_k = 0$，则不确定性贡献为0

### 条件四：
**不确定的可加性**


从这四个条件可以得到交叉熵公式
### 步骤一：
推导等概率的熵
N=2时，$H(1/2,1/2) = log2 = 1$ ，满足条件
N=4时，$H(1/4...) = log4 =2$，满足条件

### 步骤二：
用可加性推导
从二分类入手，$P=(p,1-p)$ ，使用多次等概率来选择模拟
假设选择$p =\frac{k}{M}$，也就是从M个等概率事件之中选择k个事件，结果就是二分类之一。
1-p同理
所以得到
$$logM =H(p,1-p)+plogK+(1-p)log(M-k)$$
意思就是将这M个事件看成独立事件的熵，根据**可加性**，看成两个类别的熵+两个类别内部的各自的熵
所以
$$H(p,1-p) = logM - plogK-(1-p)log(M-k)$$
可以推导出
$$H(p,1-p) = plogp - (1-p)log(1-p)$$
证毕。

### 步骤三：
推广到多分类，实际上就是在不断地分堆的过程，最后依旧可以推出这个结论

## 交叉熵的数学公式

交叉熵以及从不确定性转型为**近似**了，推导逻辑有所不同
交叉熵由**KL散度**出发
### KL散度

KL散度为了解决**量化一个概率分布近似另一个概率分布的“非对称信息损失”** 问题
熵只能解决单一分布的不确定性，但是无法衡量两个概率的分布偏差
尤其是这种偏差的**非对称性**

从**信息编码**的角度，量化用Q作为编码方案去编码P的样本时，比最佳编码方案（P）多消耗的平均**比特信息**。

#### 香农的1/N
在前面讲过，香农认为，对于等概率事件，概率为1/N，则使用的比特数为$$length = logN = -log1/N = -log(p)$$
推导出，对于一个概率为p的事件，最优编码长度为
$$length = -log(p)$$
这一结论的本质是**概率和编码长度成反比**

#### 额外编码长度
现在以及有了标准答案**P分布**，他是基于某种情况之下，最优的编码分布。那么现在我们使用**Q分布**去近似他，现在就需要计算，我们的Q浪费了多少额外的编码来拟合P分布

也就是$$\sum-log(q_i)-(-log(p_i)) = \sum{log\frac{p_i}{q_i}}$$
#### 联合P分布
对于P，所有事件都有一个额外偏差，我们需要平均起来，也就是加权P之中每一个事件发生的概率
$$KL(p//Q) = \sum_{i=1}^{N}{p_i\cdot(log\frac{p_i}{q_i})}$$

#### 数学合理性
可以推导得出，有特性**非负性，不对称性**。

### KL散度+交叉熵
因此，KL散度通过**编码长度**的中间概念，得到了
**在所有的编码都是最优编码的条件下，能够通过编码长度的比较 来 转为概率之间的比较，推导出能够适用于概率的KL散度**。

实际上，我们观察到有
$$CrossEntroy(X,Y) = H(X)+KL(X//Y)$$
$$\sum_{i=1}^{N}p_i\cdot{log(q_i)} = \sum_{i=1}^{N}p_ilog(p_i) + \sum_{i=1}^{N}{p_i\cdot(log\frac{p_i}{q_i})}$$
因此，使用KL散度和交叉熵函数是两种不同的场景
使用交叉熵函数时考虑到
- 分类头只会输出一个结果，需要分散的分布
- 需要拟合真实的分布





