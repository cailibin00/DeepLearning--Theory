在机器学习之中，我们要把获得的文本变成机器码以供学习
那么首先，要对文本进行分词处理，具体来说，就是获得需要编码的最小文本单位
后面embedding就是将这些最小文本每一个作为一个整体来编码
值得注意的是：最原始的文本处理方式就是直接划分成为char / word 
- 缺点：char需要处理的序列长度太长。word不能灵活的处理单词变形，词汇表可能会爆炸
- 优点：char将词汇表保持在很小的尺寸。word便于处理和理解，上下文长度不长
我们的目的是找到一种分词办法，使得它比word更加高效，比char更能够理解。
也就是word的基础上，获得处理复杂单词的能力，能够应对单词变形
## 目录
- [[#**一、预分词算法**]] 
- [[#**二、BPE (Byte Pair Encoding) 分词算法**]]   
- [[#**三、WordPiece分词算法**]]
- [[#**四、Word2Vec词嵌入**]]     -> [[#skip-gram模型]]  +  [[#FastText 模型词向量表示详解]]
- [[#**五、GloVe词嵌入**]]

- 附：[[#^CRF]]模型讲解
	 [[#skip-gram模型]]模型讲解

## **一、预分词算法**
预分词是针对一些特殊的语言
- 在最常用的英文，天然存在分词，也就是空格。
- 中文，日文等语言，所有的单词和字符链接在一块，需要通过预先的分析来得到类似于英文的单词（最小处理单位格式），之后才能使用类似于英文的BPE算法等算法。。。
对于中文服务器选手，当然要明白对于中文的处理！！

**关键挑战**：
- 歧义切分：`"结婚的和尚未结婚的"` → `结婚/的/和/尚未/结婚/的` vs `结婚/的/和尚/未/结婚/的`
- 未登录词识别：`"双减政策"`（新词）
- 专有名词保留：`"北京市海淀区"`（地名）

### （1）使用查表法（MM）
这是根据词汇表来分词，优缺点都是非常明显的。
- 策略：使用百万级词条**贪婪匹配**最长的匹配词条
- **示例**：  
    词典 = `["北京大学", "北京", "大学"]`  
    输入：`"北京大学"` → 输出：`["北京大学"]`（优先匹配最长词）
- **优点**：
	- 能够匹配大部分单词
	- 能够匹配专有名词
- **缺陷**：
    - 无法处理未登录词（`"量子计算"`→拆为`["量","子","计","算"]`）
    - 歧义场景失效：`"使用户满意"` → 错误匹配`["使用", "户", "满意"]`

### （2）基于统计的序列标注（CRF/HMM）
- 策略：将分词问题变成学习中文**单词边界分类**问题，需要借助深度学习

#### （2.1）思路突破

1. token和label问题：
	 - **标签体系**（BIES）：
	    - `B`：词语起始字
	    - `I`：词语中间字
	    - `E`：词语结束字
	    - `S`：单字词
	    
	- **特征工程：**
		- 训练字嵌入向量，能够使用skip-gram等模型

2. 模型设置：
	-  **编码器（Encoder）**：学习字符的上下文表示
	    - 主流选择：**BiLSTM**（捕捉长距离依赖）或 **Transformer**（并行高效）
	    
	- **解码器（Decoder）**：预测每个字符的标签
	    - **SoftMax**：独立预测每个位置标签（忽略标签间依赖）
		- **CRF层**：**必选组件**！强制标签转移合法（如I不能接S）

```mermaid

graph LR
A[输入字符] --> B[字符嵌入层]
B --> C[双向上下文编码器] 
C --> D[标签解码器]
D --> E[边界标签序列]
```

#### （2.2）全流程分析+model

 **步骤1：数据预处理**
- **输入文本**：`"人工智能改变世界"`
    
- **字符级拆分**：`["人", "工", "智", "能", "改", "变", "世", "界"]`
    
- **标签标注**（BIES）:

| 字符： | 人   | 工   | 智   | 能   | 改   | 变   | 了   | 世   | 界   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 标签  | B   | E   | B   | E   | B   | E   | S   | B   | E   |

**步骤2：特征工程**
- **字符嵌入（Char Embedding）**：
    - 初始化：随机向量 或 预训练字向量（如中文Word2Vec）

**步骤3：模型构建**
```python
class BiLSTM_CRF(nn.Module):  
    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):  
        """  
        初始化模型。  
  
        参数:  
        - vocab_size: 词汇表的大小。  
        - tag_to_ix: 标签到索引的映射字典。  
        - embedding_dim: 词嵌入的维度。  
        - hidden_dim: LSTM隐藏层的维度。  
        """        super(BiLSTM_CRF, self).__init__()  
        self.embedding_dim = embedding_dim  
        self.hidden_dim = hidden_dim  
        self.vocab_size = vocab_size  
        self.tag_to_ix = tag_to_ix  
        self.tagset_size = len(tag_to_ix)  
  
        # 1. 词嵌入层  
        self.embedding = nn.Embedding(vocab_size, embedding_dim)  
  
        # 2. BiLSTM层  
        #    - input_size: embedding_dim  
        #    - hidden_size: hidden_dim // 2 (因为是双向的，两个方向拼接)  
        #    - num_layers: 1        #    - bidirectional: True        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,  
                            num_layers=1, bidirectional=True)  
  
        # 3. 线性层  
        #    将BiLSTM的输出映射到标签空间，得到发射分数  
        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)  
  
        # 4. CRF层  
        #    使用 torchcrf 库  
        self.crf = CRF(self.tagset_size, batch_first=True)  
  
    def _get_lstm_features(self, sentence):  
        """  
        此函数从输入句子中提取 BiLSTM 特征（发射分数）。  
        """        # 词嵌入  
        embeds = self.embedding(sentence).view(len(sentence), 1, -1)  
        # LSTM 前向传播  
        lstm_out, _ = self.lstm(embeds)  
        # 调整形状以匹配线性层  
        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)  
        # 映射到标签空间  
        lstm_feats = self.hidden2tag(lstm_out)  
        return lstm_feats  
  
    def forward(self, sentence, tags, mask=None, reduction: str = 'sum'):  
        """  
        计算CRF层的负对数似然损失。  
  
        参数:  
        - sentence: 输入的句子序列 (tensor of word indices)。  
        - tags: 真实的标签序列 (tensor of tag indices)。  
        - mask: 句子的掩码，用于处理padding (1表示真实词，0表示padding)。  
        - reduction: 损失的聚合方式 ('sum', 'mean', 'token_mean')。  
  
        返回:  
        - 损失值 (tensor)。  
        """        # 从BiLSTM获取发射分数  
        # 形状: (seq_length, num_tags)  
        emissions = self._get_lstm_features(sentence)  
  
        # 将形状调整为 (batch_size, seq_length, num_tags) 以匹配CRF层要求  
        # 在这个例子中，batch_size = 1  
        emissions = emissions.unsqueeze(0)  
        tags = tags.unsqueeze(0)  
  
        if mask is not None:  
            mask = mask.unsqueeze(0).bool()  
  
        # 调用CRF层的 forward 方法计算损失  
        # 注意：CRF层返回的是负对数似然损失，所以我们需要取其相反数  
        loss = -self.crf(emissions, tags, mask=mask, reduction=reduction)  
        return loss  
  
    def decode(self, sentence, mask=None):  
        """  
        使用维特比算法解码，找到最优的标签序列。  
  
        参数:  
        - sentence: 输入的句子序列 (tensor of word indices)。  
        - mask: 句子的掩码。  
  
        返回:  
        - 最优路径的分数和路径本身 (list of tag indices)。  
        """        # 从BiLSTM获取发射分数  
        emissions = self._get_lstm_features(sentence)  
        emissions = emissions.unsqueeze(0)  # 增加 batch 维度  
  
        if mask is not None:  
            mask = mask.unsqueeze(0).bool()  
  
        # 调用 CRF 层的 decode 方法  
        # 它会返回一个列表，其中包含每个序列的最优标签路径  
        best_path = self.crf.decode(emissions, mask=mask)  
        return best_path[0]  # 因为 batch_size=1，所以取第一个结果
```

CRF讲解 ^CRF
1. **理解**：模型对于输出的序列是有**特定顺序需求**的，例如B后面不能接上I，I后面不能接上E
	根据现有的知识，我们在序列处理之中学到了CRF，也就是对于标签输出的管理
	尽管模型能够很好的预测，但是，我们能够设置一些限制条件，这些条件当然是可以列举出来的（这是重点）进而来嵌入模型，惩罚模型对于不合理序列的预测

2. **理论讲解**：**CRF层的作用就是学习这些标签之间的依赖关系和约束**。它会学习到一个“**转移分数**”矩阵，该矩阵定义了从一个标签转移到另一个标签的合理性。
	
	- **高分转移**：`B -> I` (非常合理)
	- **低分（甚至负分）转移**：`B -> S` (不合理，不允许出现)
	
	通过引入这些约束，CRF层可以确保模型最终输出的标注序列是逻辑上通顺的，而不仅仅是局部最优选择。

3. **作用原理**：
	CRF层主要做两件事：

	-  **计算损失 (Loss Calculation)**：在训练阶段，CRF层不仅仅考虑模型对单个词的预测（这部分通常来自LSTM的输出，我们称之为 **发射分数 Emission Score**），还会结合标签之间的 **转移分数 (Transition Score)**。   ->     它会计算出所有可能的标注路径的总分，并使用最大似然估计来最大化“正确”标注路径的分数，同时最小化其他所有“错误”路径的分数。这使得模型在训练时就学会了标签间的转移规则。
    
	-  **解码/预测 (Decoding)**：在预测阶段，当给定LSTM的输出（发射分数）后，CRF层不再是简单地为每个词选择概率最高的标签。相反，它会使用高效的 **维特比算法 (Viterbi Algorithm)**，结合发射分数和已经学好的转移分数，**在所有可能的标注序列中，找出一条总分最高的路径作为最终的预测结果**。这个过程保证了输出序列的合法性和最优性。

4. **数学公式**：
	 发射分数，就是正常的损失，这个时候模型会输出所有标签的概率
	 转移分数，通过转移矩阵，从**标签** 学习该路径正确的排列顺序，并且同样使用最大似然算计算损失，从而得到最好的矩阵
	 
$$Score(x,y) = \sum^{n}_{i=1}{Emit(x_i,y_i)} + \sum_{i=1}^{n-1}Trans(y_i,y_{i+1})$$
$$最大化预测目标  \ P(Y|X)= \frac{exp(Score(X,Y))}{\sum_{Y'}{exp(Score(X,Y'))}} = \frac{正确路径的得分}{所有其余路径的总得分}$$
$$损失函数： Loss = -logP(Y|X ) = - (Score(x,y) - log(Z)) ， Z是所有Score的总和$$
$$前向算法： \alpha_t(X_i) = \sum_{j=0}^{n-1}\alpha_{t-1}(X_j)\ P(X_i|X_j)\ P(Y^t|X_i)
$$
- `t` 是对应的层数，`α(X)` 是对应层数的以 X 结尾的概率 `Y^t` 是对应层数的label
假设标签索引：B=0, I=1, E=2, S=3，**转移矩阵为：**

| 当前标签\下一标签 | B(0) | I(1) | E(2) | S(3) |
| --------- | ---- | ---- | ---- | ---- |
| **B(0)**  | -∞   | 0.8  | 0.2  | -∞   |
| **I(1)**  | -∞   | 0.6  | 0.4  | -∞   |
| **E(2)**  | 0.7  | -∞   | -∞   | 0.3  |
| **S(3)**  | 0.9  | -∞   | -∞   | 0.1  |

> **关键点**：
> 
> - 合法转移有正值（如B→I, B→E）
>     
> - 非法转移设为负无穷（-∞），如B→B, I→B
>     
> - 数值初始化为可学习参数，训练过程中自动优化
>

总结一下，CRF层可以看作是在神经网络的输出和最终预测之间增加了一个“语法检查器”，这个检查器专门负责检查标签序列的合理性。

## **二、BPE (Byte Pair Encoding) 分词算法**

 **核心思想：** 从基础字符开始，**迭代合并**语料库中出现频率最高的**相邻符号对**，形成新的子词单元，直到达到目标词汇表大小。

### （1）**训练阶段（构建词汇表）：**

**输入：** 大型文本语料库 + 目标词汇表大小 `vocab_size`  
**输出：** **词汇表**（包含基础字符 + 合并生成的子词） + **合并规则列表**（Merge Rules）

1. **预处理与初始化：** (Tokenizer)
    - **文本归一化：** (pre_tokenizer：去除文本之中一无法识别的字符 & 规范化 & 处理空格)
        - 小写化（可选，如 `BERT` 使用，`GPT` 不使用）
        - Unicode 规范化（如 NFKC，将全角字符转半角，统一写法）
        - 清理非法字符、控制字符
        - **处理空格：** 将空格替换为特殊符号" `▁` "(U+2581) 或 "` ,`"  。**这是关键！** 它让算法能区分单词边界，尤其对无空格语言（如中文）至关重要。
            - 示例：`"natural language"` -> `"▁natural ▁language"`
                
    - **拆分基础单元：**
        - 将归一化后的文本拆分为**字符级**序列（包括 `▁`）。
            - 示例：`"▁natural"` -> `['▁', 'n', 'a', 't', 'u', 'r', 'a', 'l']`
                
    - **初始化词汇表：**
        - 统计所有**唯一字符**（包括 `▁`）作为初始词汇表 `Vocab`。
        - 添加必要的 **特殊Token**：
            - `<unk>`：未知词
            - `<pad>`：填充
            - `<s>`/`</s>`：句子开始/结束 (可选)
            - `<mask>`：掩码 (BERT)
        - 此时 `Vocab = {'▁', 'n', 'a', 't', 'u', 'r', 'l', 'g', 'p', 'o', 'c', 'e', 's', 'i', ... , '<unk>', '<pad>', ...}`
            
2. **迭代合并（核心循环）：**(获得核心的分词规则)
    - **统计相邻符号对频率：**
        - 遍历整个语料库，统计**当前词汇表下**所有**相邻符号对**（bigram）出现的频率。
        - 以初始状态（字符级）处理 `"▁natural"`：
            - 符号序列：`['▁', 'n', 'a', 't', 'u', 'r', 'a', 'l']`
            - 相邻对：`('▁','n')`, `('n','a')`, `('a','t')`, `('t','u')`, `('u','r')`, `('r','a')`, `('a','l')`
        - 假设 `('a','t')` 在整个语料中出现频率最高（如 1500 次）。
            
    - **合并最高频对：**
        - 将最高频符号对 `('a','t')` **合并**成一个新符号 `"at"`。
        - 将 `"at"` **加入**词汇表 `Vocab`。
        - **更新语料库：** 将所有出现 `'a'` 后紧跟 `'t'` 的地方替换为 `"at"`。
            - `"▁natural"` 更新为：`['▁', 'n', 'at', 'u', 'r', 'a', 'l']`
            - `"▁processing"`（假设存在）可能变为：`['▁', 'p', 'r', 'o', 'c', 'e', 'ss', 'i', 'ng']` → `['▁', 'p', 'r', 'o', 'c', 'e', 'ss', 'ing']` (如果 `('i','ng')` 也被合并)
                
    - **重复：**
        - 重新统计当前符号序列的相邻对频率。
        - 例如，在新序列 `['▁', 'n', 'at', 'u', 'r', 'a', 'l']` 中，新的相邻对有 `('▁','n')`, `('n','at')`, `('at','u')`, `('u','r')`, `('r','a')`, `('a','l')`。
        - 假设 `('n','at')` 现在频率很高（因为 `"nat"` 是常见组合），合并为 `"nat"` 加入词汇表。
        - 更新语料：`"▁natural"` -> `['▁', 'nat', 'u', 'r', 'a', 'l']`
            
    - **终止条件：** 循环执行，直到：
        - 词汇表大小 `len(Vocab)` 达到预设的 `vocab_size`。
        - 或没有更多可合并的相邻对（频率低于阈值）。
            
3. **最终输出：** （当前的tokenizer 已经配置了pre_tokenize  和 合并规则 两个算法，足够处理文本）
    - **词汇表 (Vocab)：** 包含所有基础字符、合并生成的子词、特殊Token。
    - **合并规则列表 (Merge Rules)：** 按合并顺序存储所有合并操作。**这是BPE的核心！**
        - 示例规则：`('a','t') -> 'at'`, `('n','at') -> 'nat'`, `('u','r') -> 'ur'`, ...
            
-  detail：对于空格在分词中的重要作用讨论：
	- 英文：
		1. 事实就是，分词必须建立在每一个单词之中，否则模型对于跨单词token的理解毫无意义，所以我们需要将分词建立在单词层面，意味着我们要分离每一个单词
		2. 从算法的角度思考，我们搞清楚思路：我们希望能够在单词层面进行编码，使得编码之后的token具有能够代表输入输出的实际含义，那么使用什么代表空格呢？实际上就是一些后缀，例如`ly` , `cal` , `tion` , 那么模型如何利用这些后缀来输出空格呢？实际上在单词最后添加显式分割符号，不就能够使得模型学习到输出空格的方式？eg : 对于单词 `apple juice ` 分词成为 `["apple</w>" , "app</w>"]`  -> `['a'...'e</w>']` and `['a', "p",'p</w>']` -> `['app' , 'le</w>'] ['app</w>']` 也就是模型能够知道，这个app是一个前缀，之后不应该输出空格休止，而后者虽然字面上也是app，但是它包含了休止符，可以作为一个whole单词或者后缀来看，这样就实现了没有显式后缀带来的难以区分的问题
		3. 在解码的时候，也能够根据`</w>` 解码得到空格作为文本
	 -  中文
		1. 在中文之中，并没有空格这一说，现在就要聚焦如何对于文本预处理，使得文本能够变成一个一个词组，以便我们添加后缀
		2. pre-tokenizer 直接影响最终编码的质量，已知的处理：简体字繁体字归一化，复杂文字的特殊处理，下面将开设章节讲解预分词的算法 ，请查找[[#目录]]
		

### （2）**编码阶段（对新文本分词）：**

**输入：** 新句子 + 训练好的词汇表 `Vocab` + 合并规则 `Merge Rules`  
**输出：** Token序列（字符串 或 ID）

1. **预处理：** 应用与训练时相同的归一化（小写、NFKC）和**空格替换**（ -> `▁`）。
    - 示例输入：`"Natural language processing is fun!"`
    - 归一化+空格处理：`"▁natural ▁language ▁processing ▁is ▁fun !"`
        
2. **拆分为字符序列：** `['▁','n','a','t','u','r','a','l','▁','l','a','n','g','u','a','g','e','▁','p','r','o','c','e','s','s','i','n','g','▁','i','s','▁','f','u','n','!']`
	
3. **应用合并规则（贪婪最长匹配）：**
    - 按**合并规则在训练中出现的顺序**（或按子词长度从长到短），尝试将当前序列中的符号**尽可能合并**成词汇表中存在的子词。
    - **关键逻辑：**
        - 遍历 `Merge Rules` 列表（按训练时的合并顺序）。
        - 对当前序列，查找是否存在该规则对应的符号对。
        - 如果存在，将其合并。
        - **重复**应用所有规则，直到无法再合并。
            
    - **示例合并过程（简化）：**
        - 规则1：`('s','s') -> 'ss'` → 处理 `"processing"` 中的 `['s','s']` -> `['ss']`
        - 规则2：`('i','n') -> 'in'` → 处理 `"processing"` 中的 `['i','n']` -> `['in']` (但 `'in'` 可能已在Vocab)
        - 规则3：`('in','g') -> 'ing'` → 处理 `['in','g']` -> `['ing']`
        - 规则4：`('p','r') -> 'pr'`, `('pr','o') -> 'pro'`, `('pro','c') -> 'proc'`, `('proc','e') -> 'proce'` ... 最终可能合并出 `'processing'` 作为一个Token（如果它在Vocab中）。
        - 对于 `"natural"`：应用规则 `('a','t') -> 'at'`, `('n','at') -> 'nat'`, `('nat','u') -> 'natu'`, `('u','r') -> 'ur'` → 最终可能拆分为 `['▁','natural']`（如果 `'natural'` 在Vocab）或 `['▁','nat','ural']`。
            
4. **处理未登录词：** 如果最终序列中存在不在 `Vocab` 中的符号（如罕见拼写错误），用 `<unk>` 替换。
    
5. **输出Token序列：**
    
    - 字符串Tokens：`['▁natural', '▁language', '▁processing', '▁is', '▁fun', '!']`
        
    - Token IDs：通过查词汇表转换为整数序列，如 `[105, 42, 987, 25, 76, 7]`

```python
from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers  
  
# 1. 初始化一个 BPE Tokenizer
tokenizer = Tokenizer(models.BPE())  
  
# 2. 设置预处理器：处理空格、小写、规范化  
# 或更通用：ByteLevel（自动处理空格、小写、Unicode）  
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)  # 添加▁ 

# 3. 设置解码器：将 ▁ 转回空格，合并字节  
tokenizer.decoder = decoders.ByteLevel()  # 或 decoders.WordPiece(prefix='▁')  

# 4. 设置训练器  
trainer = trainers.BpeTrainer(  
    vocab_size=30000,  
    special_tokens=["<unk>", "<pad>", "<s>", "</s>", "<mask>"],  
    min_frequency=2,                     # 忽略低频词  
    show_progress=True,  
    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()  # 初始包含256字节  
)  

# 配置数据集和训练集，开始模型训练
files = ["E:\\code\\动手学深度学习\\data\\timemachine.txt"]  
tokenizer.train(files, trainer=trainer)  
  
# 5. 保存与加载  
tokenizer.save("my_bpe_tokenizer.json")  
tokenizer = Tokenizer.from_file("my_bpe_tokenizer.json")  
  
# 6. 使用  
text = "Natural language processing is fun!"  
encoding = tokenizer.encode(text)  
print(encoding.tokens)  # ['▁Natural', '▁language', '▁processing', '▁is', '▁fun', '!']  
print(encoding.ids)     # [105, 42, 987, 25, 76, 7]
```


### （3）优点缺点：
-  优点：快速
-  缺点：基于统计学来进行分词，实际上不能够很好理解单词，
	 一是只能得到统计过的，在对于新词或者专有名词的处理比较欠缺。并且低频词会被筛选，可能导致**过度拆分**的语义丢失。
	 二是不能理解跨单词短语
	 三是在第一次或第二次合并的时候，元素都是字符级别的，这个时候的合并是针对于频次来合并的，不能全局的看到单词，可能存在字符级别的合并曲解了语法。例如`interesting` ，里面的`ti` 并没有实际含义。但是可能由于频率很高，被识别成为字符对。

## **三、WordPiece分词算法**

 **WordPiece 与 BPE 的本质区别**

| **特性**   | **BPE**  | **WordPiece**      |
| -------- | -------- | ------------------ |
| **合并目标** | 最高频的字符对  | **最大化语言模型似然**的字符对  |
| **选择标准** | 频率统计     | 概率增益（Δ Likelihood） |
| **训练方式** | 确定性的贪心合并 | 概率驱动的合并策略          |

> **关键创新**：WordPiece 用概率评估合并价值，而非单纯依赖频率。

### （1）算法

- 构建初始的词汇表一步，是和BPE完全相同，包括拆分成为字符级别 + 添加特殊字符标记
- **合并迭代** ： 
	- BPE核心逻辑：抓住相邻的最大概率的“对”进行合并。然而忽略了一个问题，有时候这样基于概率的合并不能很好的表现语义，比如A 和 B 在文本之中是关键词，大量出现，但是他们两个没有组合意义，但是由于大量出现就会导致他们相邻的概率变得很高，从而被统计
	-  利用公式 $$ΔL(u,v) = log P(uv) - [log (P(u) *P(v))] $$ $P(uv)$ 表示在文本中连续出现的概率，$p(u)和P(v)$ 表示单个字符出现在文本之中的概率$$Score = \frac{P(uv)}{P(u)P(v)} $$$$ΔL(u,v) = log(Score)$$
	 这个分数之际上表示了这个短语对于这两个字符的重要性，能够一定程度上表现语义！
	 选择最大的一堆 u，v 进行合并

### （2）细节

**前缀标记（`##` 符号）**
- **作用**：标识非词首子词（如 `##ing`）
- **目的**：避免歧义（如 `"ing"` 作为独立词 vs 后缀）
`[ "un", "##able", "##t", "re", "##ify", "[UNK]" ]`


**低频词语**
- **MIN_COUNT** 经验值：
    
    - 中文：5-10（BERT中文版用5）
        
    - 英文：2-5
        
- **Google原始实现**：`min_count=10`（见BERT源码）

```python
from tokenizers import Tokenizer, models, trainers

# 1. 初始化 WordPiece 模型
tokenizer = Tokenizer(models.WordPiece(max_input_chars_per_word = 16))
# 预处理器
tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()

# 2. 配置训练器
trainer = trainers.WordPieceTrainer(
    vocab_size=30000,
    special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
)

# 3. 训练词表
tokenizer.train(files=["corpus.txt"], trainer=trainer)

# 4. 编码文本
output = tokenizer.encode("Hello! 你好吗?")
print(output.tokens)  # 输出：["[CLS]", "hello", "!", "[UNK]", "[UNK]", "[UNK]", "?", "[SEP]"]
```



## **四、Word2Vec词嵌入**
承接前文我们既然已经分词成功，那么接下来编码
编码要遵循一下原则：
- 意思相近的单词点积大，反之亦然

### （1）下采样
高频词例如the , an , a 在文本，没有很多实际含义，对于他们的文本理解通常要配合实词，也就是说，实词才是最好的训练素材。
如果不丢弃部分高频词，就会出现模型在高频词上投入大量训练成本，但是效果不佳
因此要下采样，丢弃的概率是出现频率的反比
```python
def subsample(sentences, vocab):
    """下采样高频词"""
    # 排除未知词元'<unk>'
    sentences = [[token for token in line if vocab[token] != vocab.unk]
                 for line in sentences]
    counter = d2l.count_corpus(sentences)
    num_tokens = sum(counter.values())

    # 如果在下采样期间保留词元，则返回True
    def keep(token):
        return(random.uniform(0, 1) <
               math.sqrt(1e-4 / counter[token] * num_tokens))

    return ([[token for token in line if keep(token)] for line in sentences],
            counter)

subsampled, counter = subsample(sentences, vocab)
```

### （2）模型的设置
#### skip-gram模型
这是一个根据上下文来获得词向量关联度的模型
- 核心思想：
	- 在词x规定的上下文窗口之中，要是出现了词y，那就认为，词y和x具有相关性，那么xy词向量的点积应该变大
- 核心算法：
	- 1. 选择一个中心词，左右分别选取N个上下文单词
	- 2. 数学建模为：$P (...x_{i-2},x_{i-1},x_{i+1},x_{i+2}...| x_i) = \prod_{k=i-n}^{i+n}P(x_k|x_i)$ 
	- 3. 理解为出现中心词时，上下文词出现的概率，要是非常准确那就是1
	- 4. 模型表示为：使用**点积**来作为标准，$P(x_k|x_i) = \frac{exp(x_k^T x_i)}{\sum_{k=i-n}^{k=i+n}{exp(x_k^T x_i)}}$ 类似交叉熵的思想，要是出现了，P就要变大，点积就要变大，那就意味着关联性增加了
	- 5. 这个时候，模型想要优化的函数为$j(\theta)=- \prod_{t=1}^{T}{\prod_{-w+t<j<w+t}P(x_j|x_t)}$ 
- 算法改进：
	- Q：由于计算softmax一步要计算对于所有向量的点积，**计算复杂度过高**
	- 数学建模为sigmoid函数 $P(D=1|x_c,x_o) =\frac{1}{1+exp(-u_o^T  v_c)}$ 表示为是上下文的概率
	- 因此，优化的函数为$j(\theta)=- \prod_{t=1}^{T}{\prod_{-w+t<j<w+t}P(D=1|x_j，x_t)}$ 
	- 还要考虑一个重要思想，也就是**负样本**，否则模型会向着全部向量都有关联的方向发展
	- 对于每一个正样本，随机采样$K$个负样本，集合称为$N_t$
	- 数学建模更新为$P(x_k|x_i) =P(D=1|x_j,x_t)\prod_{k=1,w_k~N_t}^{K}{P(D=0|x_k,x_t)}$
损失函数为：
$$\sum^{T}_{t=1}{\frac{-\log(P)}{T}} = \sum_{t=1}^{T}\frac{-log(Sigmoid(u_j^Tv_t)) - \sum_{k=1,w_k∈N_t}^{K}log(1-Sigmoid(w_k^Tv_t))}{T}$$
最终，对于单个中心词，我们要使得对于上下文的联合概率最大，反向传播优化词嵌入的权重
在实现的时候，把没有掩码的作为正向样本，掩码的作为负向样本，也就是二分类问题

##### 试验结果
- 在BPE分词的结果上的训练
![[BPE_subsample.png|500]]
- 在没有编码上的损失函数
![[word.png|500]]
- 在WordPiece上训练的损失函数
![[WordPiece.png|500]]
我们发现在WordPiece之中分词的效果最显著

#### FastText 模型词向量表示详解
**如何通过词的子字（subword）信息来构建词向量**。这与传统的词向量模型（如 Word2Vec 中的 Skip-gram 或 CBOW）有所不同，传统模型通常将每个词视为一个独立的单元。
##### 1. 引入子字特征（Subword Features）

图片首先以单词 "where" 为例，解释了如何获取一个词的子字特征。
- 特殊字符 < 和 >：
	 为了区分一个词的开头和结尾，FastText 会在词的左右两边添加特殊字符 < 和 >。
     例如，对于单词 "where"，它会变成 `<where>`。
    - **作用：** 这有助于模型识别词的边界，并区分前缀和后缀。例如，"ing" 作为后缀和 "ing" 作为独立的词，其含义可能不同。
- 子字 n-gram：
     接下来，FastText 从这个添加了特殊字符的词中提取固定长度的子字 n-gram。图片中举例 n=3，即提取长度为3的子字。
     对于 `<where>`，当 n=3 时，提取的子字有：
	 - `<wh` + `whe` + `her` + `ere` + `re>`  +  (特殊字词)`<where>`
- 子字BPE：
	 他抛弃了n-gram的计算办法，而是利用我们已知的算法BPE，这样极大拓展了分词的灵活性，能够表示不同长度的子字，也保留了基于统计学得到的词义
##### 2.词向量构建
- $G_w​$：词 w 的所有子字集合
     “在fastText中，对于任意词 w，用 $G_w​$ 表示其长度在3和6之间的所有子字与其特殊子字的并集。”
    - **解释：** 这意味着对于一个词 w，我们会提取所有长度在3到6之间的 n-gram 子字，并且还会包含完整的词本身（特殊子字）。所有这些子字构成了集合 $G_w$​。这个长度范围（3到6）是一个常见的设置，但可以根据任务和语料库进行调整。
- $Z_g$​：子字 g 在词典中的向量
     图片中提到：“假设 $Z_g$​ 是词典中的子字 g 的向量”。
    - **解释：** FastText 在训练时，会为词典中的每一个子字（包括各种 n-gram 和完整的词）学习一个对应的向量 $Z_g$​。这些子字向量是模型的基本构建块。
- $V_w$​：对于词 w 
     构建词向量 $V_w$​ 的公式：
     $$V_w​=​\sum_{g∈G_w}Z_g$$
    - **解释：** 这个公式表明，一个词 w 的最终词向量 $V_w$​ 是其所有子字 $g∈G_w$​ 对应的子字向量 $Z_g$ 的**简单求和**。
        
- **核心思想：** **权重共享**，这就是 FastText 与传统词向量模型最大的不同之处。传统模型直接为每个词学习一个独立的向量。而 FastText 认为，一个词的含义可以通过其组成部分的子字来表示。通过将子字向量相加，FastText 能够：
    1. **处理未登录词（Out-of-Vocabulary, OOV）：** 如果一个词在训练集中没有出现过，但它的子字在训练集中出现过，FastText 仍然可以通过其子字向量的和来构建这个词的向量，从而对未登录词有更好的泛化能力。
    2. **捕捉词缀信息：** 比如 "running" 和 "walked" 都有动词词根，通过子字特征可以捕捉到这种形态学上的相似性。
    3. **减少模型参数：** 相比于为每个词学习一个独立向量，为子字学习向量可以大大减少需要学习的参数数量，尤其是在词汇量很大的情况下。
        

## **五、GloVe词嵌入**

全局向量的词嵌入（Global Vectors for Word Representation，简称 GloVe）是一种用于学习词向量的**无监督学习算法**。GloVe 的目标是生成能够捕捉词语之间语义和句法关系的词向量，**使得语义相似的词在向量空间中彼此靠近**。

### （1）解决的问题

在 GloVe 出现之前，主流的词向量学习方法主要有两类：

1. **基于局部上下文窗口的方法（如 Word2Vec 的 Skip-gram 和 CBOW）：**
    - **优点：** 能够捕捉词语的局部上下文信息，在语义相似性任务上表现良好。
    - **缺点：** 训练过程是基于局部窗口的迭代，没有直接利用**全局的词**共现统计信息。对于大型语料库，**训练效率可能受限**，且可能无法充分利用全局统计信息。
        
2. **基于全局矩阵分解的方法（如潜在语义分析 LSA）：**
    - **优点：** 直接利用了整个语料库的全局共现统计信息（通常是词-文档矩阵或词-词共现矩阵），能够捕捉词语的全局语义关系。
    - **缺点：** 无法很好地捕捉词语的细粒度语义和类比关系，因为它们通常基于降维技术，可能丢失一些局部上下文的语义细节。

GloVe 的设计目标是**结合这两类方法的优点**：既能像 Word2Vec 那样捕捉词语的局部上下文语义，又能像 LSA 那样利用全局的共现统计信息，从而在语义和句法类比任务上取得更好的表现。
### （2）核心原理（数学推导）

GloVe 的核心思想是，**词向量之间的关系应该与它们的共现概率的对数比率相关联。**

#### 3.1 词共现矩阵 X  
首先，我们定义一个词共现矩阵 X。矩阵的元素$X_{ij}​$ 表示词 $j$ 在词 $i$ 的上下文中**共现**的次数。

- **上下文窗口：** 在构建共现矩阵时，我们需要定义**一个上下文窗口**。例如，如果窗口大小为 C，那么当词 j 出现在词 i 的左右 C 个词之内时，我们就认为它们共现。
- **对称性或方向性：** 共现可以是无方向的（即 $X_{ij}$​ 等于 $X_{ij}$），也可以是有方向的（例如，只考虑词 j 出现在词 i 之后的共现）。GloVe 论文中通常使用**对称的上下文窗口**。
- **距离衰减：** 通常，GloVe 会对距离较远的共现赋予较小的权重。如果词 j 距离词 i 的距离为 d，那么共现次数可以**加权为 1/d**。 （距离近，权重高，越有效）
    
#### 3.2 共现概率 $P(j∣i)$  
基于共现矩阵 X，我们可以计算**词 j 出现在词 i 上下文中的概率**$P(j∣i)$：
$$P(j∣i)=\frac{X_{ij}}{\sum_{k}{X_{ik}}}​​
$$
其中 $∑_{k}​X_{ik}​$ 是词 i 的所有上下文词的共现总次数。
#### 3.3 共现概率比率的意义
GloVe 认为，词向量的有效表示应该能够通过简单的数学操作（如点积）来反映词共现概率的对数。更重要的是，它关注**共现概率的对数比率**。

考虑三个词：i（目标词），j（上下文词），k（探测词）。
我们观察 $P(k∣i)/P(k∣j)$ 这个比率：

- 如果词 k 与词 i 相关，但与词 j 不相关，那么 $P(k∣i)$ 会很大，而 $P(k∣j)$ 会很小，导致比率很大。
- 如果词 k 与词 j 相关，但与词 i 不相关，那么 $P(k∣i)$ 会很小，而 $P(k∣j)$ 会很大，导致比率很小（接近0）。
- 如果词 k 与词 i 和词 j 都相关，或者都无关，那么比率会接近1。
    
举例：
假设我们有词 "ice" (冰), "steam" (蒸汽), "solid" (固体), "gas" (气体), "water" (水), "fashion" (时尚)。
令i = ice  , j = steam

| 探测词 k   | $P(k∣ice)$ | $P(k∣steam)$ | $P(k∣ice)/P(k∣steam)$ | 含义                       |
| ------- | ---------- | ------------ | --------------------- | ------------------------ |
| solid   | 高          | 低            | 很高                    | 与 "ice" 相关，与 "steam" 不相关 |
| gas     | 低          | 高            | 很低                    | 与 "steam" 相关，与 "ice" 不相关 |
| water   | 高          | 高            | 1                     | 与两者都相关                   |
| fashion | 低          | 低            | 1                     | 与两者都无关                   |
这种共现概率比率的模式能够清晰地揭示词语之间的语义区别（例如“冰”和“蒸汽”在物理状态上的差异）。GloVe 的目标就是让**词向量能够捕捉这种模式**。

#### 3.4 GloVe 的目标函数推导
GloVe 模型的出发点是：词向量的点积应该与它们共现的对数概率相关。
为了满足这种结构，GloVe 提出了以下假设形式：
$$w_i^T​w_j^{'}​=logX_{ij}$$
这个简化形式是说，两个词向量的点积应该等于它们共现次数的对数。

根据上文的推导：
对于目标函数，我们希望存在一个函数 F 使得：
$$F(w_i​,w_j​,w^{'}_k​)=\frac{P(k∣j)}{P(k∣i)}​$$

其中 $w_i​,w_j​,w_k^{'}$​ 分别是词 $i,j,k$的词向量。$w_k^{'}$​ 表示上下文词的向量，GloVe 中对每个词学习两个向量：一个作为中心词的向量 $w$，一个作为上下文词的向量 $w^{'}$。
为了简化，  $P(k∣i)/P(k∣j)$ 的比率我们自然想到使用对数形式：
$$F(w_i​,w_j​,w_k^{'}​)=logP(k∣i)−logP(k∣j)$$
根据文章的假设，划出 $(w_i​−w_j​)^Tw_k^{'}$​ 的形式：
$$w_i^T​w_k^{'}​−w_j^T​w_k^{'}​=logP(k∣i)−logP(k∣j)$$

为了实现这个目标，GloVe 提出了以下损失函数（或目标函数）：
$$J=\sum_{i=1}^V\sum^V_{j=1}​f(X_{ij}​)(w_i^T​w_j^{'}​+b_i​+b_j^{'}​−logX_{ij}​)^2$$

让我们分解这个目标函数中的各项：
- V：词汇表的大小。
- $w_i​∈R^d$：中心词 i 的词向量（维度为 d），对于每个中心词处理。
- $w_j^{'}​∈R^d$：上下文词 j 的词向量（维度为 d），每个中心词要计算每一个上下文。
    - GloVe 为每个词学习两个向量：一个作为中心词的 $w_i​$，一个作为上下文词的 $w_j^{'}$​。在训练结束后，通常会将 $w_i​$ 和 $w_i^{'}$​ **相加或取平均作为最终的词向量**。
- $b_i​∈R$：中心词 i 的偏置项。
- $b_j^{'}​∈R$：上下文词 j 的偏置项。
    - 偏置项的作用是弥补 $w_i^T​w_j^{'}$​ 无法完全捕捉所有信息的情况，类似于线性回归中的截距。它们使得模型在 $X_{ij}​$为零时也能有合理的表现。
- $X_{ij}$​：词 i 和词 j 的共现次数。
- $logX_{ij}$​：共现次数的对数。如果 $X_{ij}$​=0，则 $logX_{ij}​$ 是负无穷，这在实际中需要特殊处理（通常通过 $f(X_{ij}​)$ 函数来避免）。
- $f(X_{ij​})$：**权重函数（Weighting Function）**。这是一个非递减函数，用于给不同的共现次数赋予不同的权重。
    - **作用：**
        1. **处理** $X_{ij}​=0$ **的情况：** $logX_{ij}$​ 无意义。f(0) 被定义为0，这样共现次数为0的词对就不会对损失函数产生贡献。
        2. **降低高频共现的权重：** 像 "the", "a" 这样的停用词会频繁共现，但它们的共现信息可能不如低频词对那么有区分度。f($X_{ij}$​) 可以限制这些高频共现的贡献，防止它们主导训练过程            
        3. **提升低频共现的权重：** 确保即使是低频但有意义的共现也能被模型学习到。
    
    GloVe 论文中提出的权重函数形式为：$$f(x)=(x/x_{max}​)^α\ \ \ \  if\ \ x<x_{max}\ \ else\ \  1​​$$   
    - x 代表 $X_{ij}$​。
    - $x_{max}​$ 是一个超参数，表示截断点（例如 $x_{max}$​=100）。当共现次数超过 $x_{max}​$ 时，权重恒定为1。
    - α 是一个超参数（通常取 α=0.75）。它使得权重函数在 $x<x_{max}​$ 时是非线性的，可以更好地平衡高频和低频共现。
        
目标函数的直观理解：

这个目标函数可以理解为：我们希望通过学习词向量 $w_i​,w_j^{'}$​ 和偏置项 $b_i​,b_j^{'}$​，使得它们的点积加上偏置项，尽可能地接近词 i 和词 j 共现次数的对数。平方误差项 $(...)^2$确保了这种接近性。权重函数 $f(X_{ij}​)$ 则对不同频率的共现进行加权，以优化学习效果。

奇妙之处
- 能够使用$f(x)$函数来缩放上下文影响力，这一点胜过skip-gram
- 能够看到全局的信息，建模具有考虑全局的特点，代价就是训练之前要对全文进行统计处理
- 即使建模可能导致词向量点积大于1，但是在实际使用过程之中会归一化处理，这不是一个难点
#### 3.5 训练过程

GloVe 模型通过**随机梯度下降（Stochastic Gradient Descent, SGD）或其变种（如 Adam）来优化上述目标函数。在训练过程中，模型会迭代地更新词向量 $w_i​,w_j^{'}$​ 和偏置项 $b_i​,b_j^{'}$​，以最小化损失函数 $J$。

### （3）GloVe 的优势

- **结合全局和局部信息：** GloVe 成功地将全局共现统计信息和局部上下文信息结合起来，使得生成的词向量在多种任务上表现优异。
- **高效性：** 相比于 Word2Vec，GloVe 可以直接利用预先计算好的共现矩阵，训练过程相对高效。
- **可解释性：** 目标函数基于共现概率的对数比率，这使得模型具有一定的可解释性，能够更好地理解词语之间的语义关系。
- **类比推理能力：** GloVe 向量在词语类比任务上表现出色，这表明它能够捕捉到词语之间更复杂的线性关系。

### （4）Glove的一般缺点
我的核心观点是 ： **共现 ≠ 语义关联：功能词干扰与虚假关联**

“窗口共现” 仅能捕捉 “统计上的共同出现”，但统计共现不等于语义关联：
- **功能词的干扰**：“的”“了”“是” 等高频功能词会与几乎所有内容词（如 “猫”“书”“跑”）共现，按模型假设，这些功能词会与内容词产生 “语义关联”，但实际上它们没有任何实质语义，反而会稀释内容词的向量质量（例如 “猫” 的向量会混入 “的” 的无意义信息）。（需要下采样来抛弃，但是不能完全解决问题）
- **虚假共现的误导**：某些词因文本主题**巧合**频繁共现，但无语义关联。例如在 “天气预报” 语料中，“今天” 和 “下雨” 频繁共现（模型认为关联），“今天” 和 “星期三” 也频繁共现（模型也认为关联），但 “下雨” 和 “星期三” 并无语义关联 —— 但模型可能因 “今天” 的**中介作用**，让 “下雨” 和 “星期三” 的向量接近，导致语义混淆。
- **低频词（如生僻词、专业术语）的共现次数极少**，导致模型无法为其学习到准确的语义关联。