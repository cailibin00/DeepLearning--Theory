![](file-20250930164322441.png)

### get
- 先富后富的普遍规则（改进的目的是运用尽可能多的参数，添加噪声and硬/软约束）
- 卷积的方式堆叠和直接堆叠的关系

### Noisy Top-K Gating
- 由于多专家在训练的时候，为了得到挑选特定专家计算得到结果（兼顾理解和计算效率），我们提出了稀疏的挑选专家策略。
- 我们训练的时候注意到，如果选择传统top-k机制，会导致**先富后富**问题，也就是会导致先训练的专家在后续表现上面更好，这样就导致后续选择专家的时候偏向于选择已训练的专家，导致logits在部分专家的权重很大。模型失效。（这是基于统计概率上来思考问题的）
$$ KeepTopK(v,k)_i=\left\{
\begin{aligned}
v_i \ \ \ \ \ if\ v_i\ \  is\ \  in\ topK\ \ of\ v \\
-∞\ \ \ otherwise\\
\end{aligned}
\right.
$$
$$H(x)_i​=(x⋅W_g​)_i​+StandardNormal()⋅Softplus((x⋅W_{noise}​)_i​)$$
$$  
Softplus(z)=ln(1+e^z)$$
- 其中，后面是用来加入噪声的，使用了一个新的权重$W_{noisy}$ ：它的作用是学习 “不同输入对不同专家的噪声敏感度”，softplus是为了控制噪声的强度
- 前面正态分布项生成服从 “标准正态分布（均值 0、方差 1）” 的随机数，作用是**打破门控的 “确定性偏爱”**，是一个随机源
- 二者的协作最终实现：**“随机注入（正态分布）+ 自适应强度（W_{noise}）”** 的噪声机制


### 软约束
我们针对专家设置一个损失函数（优点牵强，因为可能前面的noisy效果不太明显，所以继续添加了一个约束专家选择的损失函数），重点是约束专家选择的分散性（从侧面可以看出/正面参数训练的时候，他的规律性）
给出公式，对于一整个batch（批次）：
$$L_{importance}(X) = \alpha * CV(importance(z)...z∈专家s)$$
$$importance(z) = \sum_{x∈X}{logits(x) * num(X)}$$
$$CV(X) = \frac{方差}{平均数}$$
- importance计算就是针对于该专家，计算X之中的所有加权之和
- 目的就是为通过反差和平均值的CV（变异系数）的值的调整来拉匀专家的权重

